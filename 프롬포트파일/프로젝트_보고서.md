# 흑백요리사2 방송 효과 분석 시스템

## 📋 프로젝트 개요

본 시스템은 넷플릭스 **흑백요리사 시즌2** 방송이 출연 셰프들의 가게 리뷰 수와 서울시 유동인구에 미치는 영향을 분석하는 데이터 파이프라인 및 대시보드입니다.

---

## 🏗️ 시스템 아키텍처

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           DATA SOURCES                                   │
├─────────────────┬─────────────────────┬─────────────────────────────────┤
│  서울시 IoT API  │   캐치테이블 웹사이트  │     로컬 CSV 파일               │
│  (유동인구)      │   (레스토랑 리뷰)      │    (가게 정보)                  │
└────────┬────────┴──────────┬──────────┴───────────────┬─────────────────┘
         │                   │                          │
         ▼                   ▼                          │
┌─────────────────────────────────────────────────────────────────────────┐
│                        AIRFLOW DAGs                                      │
├─────────────────────────────┬───────────────────────────────────────────┤
│  seoul_population_collector │  catchtable_review_collector              │
│  (6시간마다 실행)            │  (매일 오전 6시 실행)                      │
└────────────┬────────────────┴─────────────────┬─────────────────────────┘
             │                                  │
             ▼                                  ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                          STORAGE                                         │
├─────────────────────────────┬───────────────────────────────────────────┤
│       로컬 CSV 파일          │           Supabase (PostgreSQL)           │
│  seoul_floating_pop_raw3.csv │  seoul_floating_population 테이블         │
│  review_count_history.csv    │  catchtable_reviews 테이블                │
└────────────┬────────────────┴─────────────────┬─────────────────────────┘
             │                                  │
             ▼                                  ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                        DASHBOARDS (Streamlit)                            │
├─────────────────────────────┬───────────────────────────────────────────┤
│  streamlit_app.py            │  streamlit_app_supabase.py               │
│  (CSV 기반 - 오프라인)        │  (Supabase 실시간 연동)                   │
└─────────────────────────────┴───────────────────────────────────────────┘
```

---

## 📁 파일 구조

```
흑백요리사/
├── 📂 dags/                              # Airflow DAG 파일
│   ├── seoul_population_collector_dag.py  # 유동인구 수집 DAG
│   └── catchtable_review_collector_dag.py # 리뷰 수집 DAG
│
├── 📂 대시보드용/                          # Streamlit 대시보드
│   ├── data_processor.py                  # 데이터 전처리 모듈 (CSV)
│   ├── supabase_data_loader.py            # Supabase 데이터 로더
│   ├── review_heatmap.py                  # 리뷰 히트맵 시각화
│   ├── population_animated_map.py         # 유동인구 지도 시각화
│   ├── streamlit_app.py                   # 대시보드 (CSV 버전)
│   ├── streamlit_app_supabase.py          # 대시보드 (Supabase 버전)
│   └── .env.example                       # 환경변수 템플릿
│
├── 📂 데이터수집code/                      # 수동 데이터 수집 스크립트
│   ├── 실시간인구데이터수집코드.py          # 유동인구 수집
│   └── 캐치테이블_식당리뷰.py               # 리뷰 크롤링
│
├── 📂 프롬포트파일/                        # 문서 및 프롬프트
│   └── 흑백요리사2_대시보드_프롬프트.md     # 분석 요구사항
│
├── 📄 seoul_floating_pop_raw3.csv         # 유동인구 데이터 (49만건+)
├── 📄 reviews_collected_20260114.csv      # 수집된 리뷰 데이터
├── 📄 캐치테이블_가게정보.csv              # 출연진 가게 정보 (118개)
├── 📄 supabase_schema.sql                 # Supabase 테이블 스키마
└── 📄 docker-compose.yaml                 # Airflow Docker 설정
```

---

## ⚙️ DAG 상세 설명

### 1. seoul_population_collector_dag.py

**목적**: 서울시 IoT 유동인구 데이터를 자동으로 수집하여 CSV와 Supabase에 저장

| 항목 | 내용 |
|------|------|
| **DAG ID** | `seoul_population_collector` |
| **스케줄** | `0 */6 * * *` (6시간마다) |
| **시작일** | 2026-01-15 |
| **재시도** | 2회 (5분 간격) |

**Task 흐름**:
```
start → collect_population_data → load_to_supabase → end
```

**주요 함수**:
- `get_seoul_api_key()`: Airflow Connection에서 Seoul API 키 조회
- `fetch_data_batch()`: API에서 1000개씩 데이터 가져오기
- `collect_population_data()`: 수집 로직 (중복 방지, CSV 저장)
- `load_to_supabase()`: Supabase REST API로 데이터 적재

**Airflow Connection 필요**:
| Connection ID | 설정 |
|---------------|------|
| `seoul_api` | Password: Seoul API Key |
| `xoosl033110_supabase_conn` | Host: Supabase URL, Password: API Key |

---

### 2. catchtable_review_collector_dag.py

**목적**: 캐치테이블 웹사이트에서 출연진 가게 리뷰 수를 크롤링하여 저장

| 항목 | 내용 |
|------|------|
| **DAG ID** | `catchtable_review_collector` |
| **스케줄** | `0 6 * * *` (매일 오전 6시) |
| **시작일** | 2026-01-15 |
| **재시도** | 1회 (10분 간격) |

**Task 흐름**:
```
start → collect_reviews → load_reviews_to_supabase → save_daily_snapshot → end
```

**주요 함수**:
- `setup_driver()`: Selenium 헤드리스 브라우저 설정
- `get_review_count()`: 단일 가게의 리뷰 수 추출
- `collect_reviews()`: 전체 가게 순회하며 리뷰 수집
- `load_reviews_to_supabase()`: Supabase에 적재
- `save_daily_snapshot()`: 일별 CSV 스냅샷 저장

---

## 📊 대시보드 기능

### 탭 1: 리뷰 히트맵
- **X축**: 방영 회차 (1회~5회)
- **Y축**: 셰프/가게명
- **색상**: 리뷰 증가율 (파랑=감소, 빨강=증가)
- **필터**: 최소 리뷰 3개 이상 가게만 표시

### 탭 2: 유동인구 지도
- **애니메이션 모드**: 일별 유동인구 변화 재생
- **비교 모드**: 방영일 전/후 7일 변화율
- **정적 모드**: 특정 날짜 유동인구
- **마커**: ★ 흑백요리사 출연 가게 위치 (호버 시 정보 표시)

### 탭 3: 상세 분석
- 개별 가게 선택 시 회차별 상세 데이터
- 방영 전/후 리뷰 수 막대 그래프

---

## 🔑 방영일 정보

| 회차 | 방영일 | 7일 전 시작 | 7일 후 종료 |
|------|--------|-------------|-------------|
| 1회 | 2025-12-16 | 2025-12-09 | 2025-12-23 |
| 2회 | 2025-12-23 | 2025-12-16 | 2025-12-30 |
| 3회 | 2025-12-30 | 2025-12-23 | 2026-01-06 |
| 4회 | 2026-01-06 | 2025-12-30 | 2026-01-13 |
| 5회 | 2026-01-13 | 2026-01-06 | 2026-01-20 |

---

## 📈 데이터 현황 (2026-01-16 기준)

| 데이터 | 레코드 수 | 기간 |
|--------|----------|------|
| 유동인구 | 497,055건 | 2025-12-09 ~ 2026-01-14 |
| 리뷰 | 7,644건 | 2025-12-09 ~ 2026-01-14 |
| 출연 가게 | 118개 | - |

---

## ⚠️ 주의사항

### 1. Airflow 관련
- Docker Compose로 Airflow 실행 필요 (`docker-compose up -d`)
- Airflow Connection 사전 등록 필수:
  - `seoul_api`: Seoul Open API 키
  - `xoosl033110_supabase_conn`: Supabase 연결 정보
- DAG 파일 수정 후 Airflow 웹 UI에서 새로고침 필요

### 2. 크롤링 관련
- Selenium Chrome Driver 설치 필요
- 캐치테이블 웹사이트 구조 변경 시 셀렉터 수정 필요
- Rate Limiting: 요청 간 1초 대기

### 3. 데이터 관련
- 5회 방영일(1/13) 7일 후 데이터는 1/14까지만 존재
- 리뷰 데이터에 "Unknown" 날짜 존재 → 전처리 시 제외
- 유동인구 VISITOR_COUNT=0 레코드 존재 → 정상 데이터

### 4. Supabase 관련
- 테이블 생성 필요 (`supabase_schema.sql` 실행)
- API Rate Limit 주의 (배치 500개씩 삽입)
- RLS 정책 비활성화 상태에서 테스트 권장

### 5. 대시보드 관련
- CSV 버전: 브라우저 새로고침 시 최신 데이터 반영
- Supabase 버전: 5분마다 자동 캐시 갱신
- `.env` 파일에 Supabase 연결 정보 필요 (Supabase 버전)

---

## 🚀 실행 방법

### 1. Airflow 실행
```bash
cd 흑백요리사
docker-compose up -d
# 웹 UI: http://localhost:8080
```

### 2. 대시보드 실행 (CSV 버전)
```bash
cd 대시보드용
streamlit run streamlit_app.py
# 접속: http://localhost:8501
```

### 3. 대시보드 실행 (Supabase 버전)
```bash
cd 대시보드용
# .env 파일 생성 후 Supabase 정보 입력
streamlit run streamlit_app_supabase.py
```

---

## 🔧 필요 패키지

```
pandas
plotly
streamlit
requests
selenium
python-dotenv
apache-airflow (Docker 이미지 사용 시 불필요)
```

---

## 📞 문의

프로젝트 관련 문의사항은 담당자에게 연락하세요.

---

*마지막 업데이트: 2026-01-16*
